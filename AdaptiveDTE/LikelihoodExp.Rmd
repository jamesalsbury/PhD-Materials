---
title: "MCMC - DTE exponential"
output: html_document
date: 'July 2022'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(survival)
library(stats4)
library(rstan)
library(ggmcmc)
library(rjags)
library(GenSA)
```

## Implementing Bayesian methods for DTE - Exponential parameterisation

We have some data $\textbf{x}= (x_{c,1},\ldots,x_{c,n}, x_{t,1}, \ldots, x_{t, m})$. We are tasked with finding the most likely values for the parameters $\lambda_c$, $\lambda_t$ and $T$ if we assume they come from the following survival functions

$$S_c(t) = \text{exp}\{-\lambda_ct\}$$

$$S_t(t) = \text{exp}\{-\lambda_ct\}\mathbb{1}_{t\leq T}+\exp\{-\lambda_cT-\lambda_t(t-T)\}\mathbb{1}_{t>T}$$

We have the relationship

$$f(t) = \frac{d}{dt}[1-S(t)]$$

Therefore, the above survival probabilities can be manipulated to give the following densities

$$f_c(t) = \lambda_c\text{exp}\{-\lambda_ct\}$$ $$f_t(t) = \lambda_c\text{exp}\{-\lambda_ct\}\mathbb{1}_{t\leq T}+\lambda_t\exp\{-\lambda_cT-\lambda_t(t-T)\}\mathbb{1}_{t>T}$$

Therefore, the joint density is

```{=tex}
\begin{align}
p(\textbf{x}|\lambda_c, \lambda_t,T) &= p(x_{c,1},\ldots,x_{c,n}|\lambda_c) \times p(x_{t,1},\ldots,x_{t,m}|\lambda_c,\lambda_t, T) \\
&= p(x_{c,1},\ldots,x_{c,n}|\lambda_c) \times p(x_{t,1},\ldots,x_{t,j}|\lambda_c) \times p(x_{t,j+1},\ldots,x_{t,m}|\lambda_c,\lambda_t, T),
\end{align}
```
where $x_{t,j}$ is the largest observed $x$ in the treatment group before T.

The joint likelihood is

```{=tex}
\begin{align}
\mathcal{L}(\lambda_c,\lambda_t, T|x_{c,1},\ldots,x_{c,n}, x_{t,1}, \ldots, x_{t, m}) &= \prod^n_{i=1} \lambda_c\text{exp}\{-\lambda_cx_{c,i}\} \times \prod^j_{k=1} \lambda_c\text{exp}\{-\lambda_cx_{t,k}\} \times \prod^m_{p=j+1} \lambda_t\text{exp}\{-\lambda_t(x_{t,p}-T)-\lambda_cT\} \\
&= \lambda_c^n\text{exp}(-\lambda_c \sum^n_{i=1}x_{c,i}) \times \lambda_c^j\text{exp}(-\lambda_c \sum^j_{k=1}x_{t,k}) \times \lambda_t^{m-j}\text{exp}\{(m-j)(\lambda_t T-\lambda_c T)-\lambda_t \sum^m_{p=j+1}x_{t,p}\}
\end{align}
```
We find the log-likelihood

```{=tex}
\begin{align}
\text{log} \mathcal{L}(\lambda_c,\lambda_t, T|x_{c,1},\ldots,x_{c,n}, x_{t,1}, \ldots, x_{t, m}) &= n \text{log}\lambda_c-\lambda_c\sum^n_{i=1}x_{c,i}+j\text{log}\lambda_c-\lambda_c\sum^j_{k=1}x_{t,k}+(m-j)\text{log}\lambda_t+(m-j)(\lambda_t T-\lambda_c T)-\lambda_t\sum^m_{p=j+1}x_{t,p} \\
&= (n+j)\text{log}\lambda_c - \lambda_c\big(\sum^n_{i=1}x_{c,i}+\sum^j_{k=1}x_{t,k}\big) +(m-j)(\text{log}\lambda_t+\lambda_t T - \lambda_c T)-\lambda_t \sum^m_{p=j+1}x_{t,p}
\end{align}
```
<!-- We can ensure that this is the correct log-likelihood with the following code -->

```{r, eval=T, echo=F}

#set.seed(24)

#Setting up the correct parameters - we would not know this in practice

lambdat <- 0.03
lambdac <- 0.09
bigT <- 5

#Sample sizes in each group
n1 <- 1000
n2 <- 1000

#When are we performing the IA?
IATime <- 30

#How long are we running the trial for in total?
totalLength <- 60

#Simulating the control and treatment data - again, we would not normally know the underlying structure, but for illustration this is okay
#Control
controlData <- rexp(n1, lambdac)
#Treatment
CP <- exp(-(lambdac*bigT))
u <- runif(n2)
treatmentData <- ifelse(u>CP, -(1/lambdac)*log(u), (1/lambdat)*(bigT*(lambdat-lambdac)-log(u)))

combinedData <- data.frame(time = c(controlData, treatmentData), group = c(rep("Control", n1), rep("Treatment", n2)))

combinedData$event <- combinedData$time<IATime

combinedData$time[combinedData$time>IATime] <- IATime

controlkm <- survfit(Surv(time, event)~group, data = combinedData)

plot(controlkm)

#Using the data to find the parameters we have defined
n <- length(controlData)
m <- length(treatmentData)
treatmentData <- sort(treatmentData)

# #Coding the log-likelihood function we derived
# nll <- function(bigTEst, lambdacEst, lambdatEst){
#   j <- sum(treatmentData < bigTEst)
#   loglikelihood <- (n+j)*log(lambdacEst)-lambdacEst*(sum(controlData)+sum(treatmentData[1:j]))+
#   (m-j)*(log(lambdatEst)+lambdatEst*bigTEst-lambdacEst*bigTEst) - lambdatEst*sum(treatmentData[(j+1):m])
#   #Returning the negative ll as this is what the mle() function expects
#  return(-loglikelihood)
# }
# 
# #Finding the maximum likelihood estimators for T, lambdac and lambdat
# #If we change the starting value of bigTEst then the estimation just settles on this initial value
# est <- stats4::mle(minuslogl = nll, start = list(bigTEst = 20,lambdacEst = 0.01, lambdatEst = 0.01), lower=  list(bigTEst=2, lambdacEst = 0.0001, lambdatEst = 0.0001), upper = list(bigTEst = 60))
# 
# #Showing the output of the mle() function
# est
# 
# 
# #Let us just look at the likelihood for T (we are assuming we know what lambdat and lambdac are here)
# 
# nllT <- function(bigTEst){
#   j <- sum(treatmentData < bigTEst)
#   loglikelihood <- (n+j)*log(lambdac)-lambdac*(sum(controlData)+sum(treatmentData[1:j]))+
#   (m-j)*(log(lambdat)+lambdat*bigTEst-lambdac*bigTEst) - lambdat*sum(treatmentData[(j+1):m])
#   #Returning the negative ll as this is what the mle() function expects
#  return(-loglikelihood)
# }
# 
# #Still not identifiable, why?
# #estT <- stats4::mle(minuslogl = nllT, start = list(bigTEst = 20), lower=list(bigTEst = 0.5), upper = list(bigTEst = 60))
# 
# #Let us look at the plot of bigT vs loglikelihood
# 
# bigTVec <- seq(1, 40, by=0.001)
# 
# estT1 <- sapply(X = bigTVec, FUN = nllT)
# 
# #Quite obvious where the minumum is, so why isn't it finding it?
# plot(bigTVec, estT1, type="l")
# 
# bigTVec[which.min(estT1)]
# 
# #Let's try optimize
# 
# optimize(nllT, interval = c(1, 60))
# 
# #Optimize works!
# 
# nlloptim <- function(x){
#   j <- sum(treatmentData < x[1])
#   loglikelihood <- (n+j)*log(x[2])-x[2]*(sum(controlData)+sum(treatmentData[1:j]))+
#   (m-j)*(log(x[3])+x[3]*x[1]-x[2]*x[1]) - x[3]*sum(treatmentData[(j+1):m])
#   #Returning the negative ll as this is what the mle() function expects
#  return(-loglikelihood)
# }


SA <- GenSA(par = c(20, 0.01, 0.01), fn = nlloptim, lower=c(0.5, 0.001, 0.001), upper=c(60, 10, 10))

MLEestimates <- SA$par


controlTime <- seq(0, 40, by=0.01)
controlSurv <- exp(-MLEestimates[2]*controlTime)
lines(controlTime, controlSurv, col = "blue")

treatmentTime1 <- seq(0, MLEestimates[1], by=0.01)
treatmentSurv1 <- exp(-MLEestimates[2]*treatmentTime1)
lines(treatmentTime1, treatmentSurv1, col = "red")

treatmentTime2 <- seq(MLEestimates[1], 40, by=0.01)
treatmentSurv2 <- exp(-MLEestimates[2]*MLEestimates[1]-MLEestimates[3]*(treatmentTime2-MLEestimates[1]))
lines(treatmentTime2, treatmentSurv2, col = "red")


```


```{r}
#Now we can try it with JAGS - set the initial values of the paramaters to be the likelihoods?

n <- n1
m <- n1+n2

modelstring="

data {
  for (j in 1:m){
    zeros[j] <- 0
  }
}

model {
  C <- 1000000
  for (i in 1:n){
    zeros[i] ~ dpois(zeros.mean[i])
    zeros.mean[i] <-  -l[i] + C
    l[i] <- ifelse(datEvent[i]==1, log(lambdac)-lambdac*datTimes[i], -lambdac*datTimes[i])
  }
  for (i in (n+1):m){                                                                                                             
    zeros[i] ~ dpois(zeros.mean[i])
    zeros.mean[i] <-  -l[i] + C
    l[i] <- ifelse(datEvent[i]==1, ifelse(datTimes[i]<bigT, log(lambdac)-lambdac*(datTimes[i]), log(lambdat)-lambdac*bigT-lambdat*(datTimes[i]-bigT)), 
      ifelse(datTimes[i]<bigT, -lambdac*datTimes[i], -lambdac*bigT-lambdat*(datTimes[i]-bigT)))
  }
  
    lambdac ~ dbeta(1,1)T(0,)
    lambdat ~ dbeta(1,1)T(0,)
    bigT ~ dnorm(6, 0.1)T(0,)
     
    }
"

model = jags.model(textConnection(modelstring), data = list(datTimes = combinedData$time, datEvent = combinedData$event, n= n, m=m), inits = list(bigT = SA$par[1], lambdac = SA$par[2], lambdat = SA$par[3]))

update(model, n.iter=1000)
output=coda.samples(model=model, variable.names=c("lambdac", "lambdat", "bigT"), n.iter = 10000)


summary(output)
plot(output)



```

```{stan output.var="ia1", eval=F, echo=F}

functions {
  real expmodel_log(vector Y, int controlLength, int treatmentLength, real lambdac, real lambdat, real bigT) {
    real loglik;
    int j;
    int n;
    int m;
    vector[controlLength] controlData;
    vector[treatmentLength] treatmentData;
    n = controlLength;
    m = treatmentLength;
    controlData = Y[1:n];
    treatmentData = Y[(n+1):(n+m)];
    j=0;
    for (i in 1:m){
      if (treatmentData[i]<bigT){
        j = j + 1;
      }
    }
   
    loglik = (n+j)*log(lambdac)-lambdac*(sum(controlData)+sum(treatmentData[1:j]))+
  (m-j)*(log(lambdat)+lambdat*bigT-lambdac*bigT) - lambdat*sum(treatmentData[(j+1):(m)]);
    return loglik;
  }
}

data {
  int controlLength;
  int treatmentLength;
  vector[controlLength+treatmentLength] Y;
}

parameters {
  real <lower=0> lambdac;
  real <lower=0> lambdat;
  real <lower=0> bigT;
}

model {
  lambdac ~ beta(1, 1);
  lambdat ~ beta(1, 1);
  bigT ~ normal(6, 1);
  Y ~ expmodel(controlLength, treatmentLength, lambdac, lambdat, bigT);
}

```

```{r, eval=F, echo=F}

lambda1 <- 0.04
lambda2 <- 0.08
bigT <- 6

#Sample sizes in each group
n1 <- 500
n2 <- 500

#Simulating the control and treatment data - again, we would not normally know the underlying structure, but for illustration this is okay
#Control
controlData <-rexp(n1, lambda2)
#Treatment
CP <- exp(-(lambda2*bigT))[[1]]
u <- runif(n2)
suppressWarnings(treatmentData <- ifelse(u>CP, -(1/lambda2)*log(u), (1/lambda1)*(-log(u)-bigT*(lambda2-lambda1))))

n <- length(controlData)
m <- length(treatmentData)
treatmentData <- sort(treatmentData)

combinedData <- c(controlData, treatmentData)

 sampling_iterations <- 1e3
 fit <- sampling(ia1,
 data = list(Y = combinedData, controlLength=n, treatmentLength=m),
 chains = 1,
 iter = sampling_iterations,
 warmup = sampling_iterations/4,
 refresh=0)
 
 summary(fit)
 
 samples <- ggmcmc::ggs(fit)
 ggmcmc::ggs_pairs(samples)
 
 ggmcmc::ggs_caterpillar(samples, thick_ci = c(0.25,0.75))
 
 ggmcmc::ggs_traceplot(samples)
```

<!-- <https://my.vanderbilt.edu/jeffannis/files/2016/06/AnnisMillerPalmeri2016.pdf> -->

We now look at the case where we have censored observations, that is at time $t_j$, we will have some observed event times, but more importantly, some unobserved event times. . We assume that the first $r$ times are observed, whilst for $x_{r+1},\ldots,x_n$, we only know that $x_i > t_j$.

In this case, the likelihood becomes

```{=tex}
\begin{align}
\mathcal{L}(\lambda) = \prod^r_{i=1}f(t)\times \prod^n_{i=r+1}S(t)
\end{align}
```
Using this, we can rewrite the original joint likelihood as

```{=tex}
\begin{align}
\mathcal{L}(\lambda_c,\lambda_t, T|x_{c,1},\ldots,x_{c,n}, x_{t,1}, \ldots, x_{t, m}) &= \prod^r_{i=1} \lambda_c\text{exp}\{-\lambda_cx_{c,i}\}\prod^n_{i=r+1} \text{exp}\{-\lambda_cx_{c,i}\}  \\
&= \lambda_c^n\text{exp}(-\lambda_c \sum^n_{i=1}x_{c,i}) \times \lambda_c^j\text{exp}(-\lambda_c \sum^j_{k=1}x_{t,k}) \times \lambda_t^{m-j}\text{exp}\{(m-j)(\lambda_t T-\lambda_c T)-\lambda_t \sum^m_{p=j+1}x_{t,p}\}
\end{align}
```
<!-- <https://ebookcentral.proquest.com/lib/sheffield/reader.action?docID=427808> -->

# Try and look at the zeros-ones trick for JAGS?

```{r, eval=F}
 
lambda1 <- 0.04
lambda2 <- 0.08
bigT <- 6

#Sample sizes in each group
n1 <- 200
n2 <- 200

#When are we performing the IA?
IATime <- 600

#How long are we running the trial for in total?
totalLength <- 60

#Simulating the control and treatment data - again, we would not normally know the underlying structure, but for illustration this is okay
#Control
controlTimes <-  rexp(n1, lambda2)
#Treatment
CP <- exp(-(lambda2*bigT))[[1]]
u <- runif(n2)
suppressWarnings(treatmentTimes <- ifelse(u>CP, -(1/lambda2)*log(u), (1/lambda1)*(-log(u)-bigT*(lambda2-lambda1))))

combinedData <- data.frame(time = c(controlTimes, treatmentTimes), group = c(rep("control", n1), rep("treatment", n2)))

combinedData$event <- combinedData$time<IATime


n <- n1
m <- n1+n2

# kmfit <- survfit(Surv(time, cens)~group, data = combinedData)
# plot(kmfit, conf.int=F, col=c("blue", "red"))

nll <- function(bigTEst){
  lambdacEst <- lambda2
  lambdatEst <- lambda1
   loglikelihood <-  sum(treatmentData<bigTEst)*log(lambdacEst)-lambdacEst*sum(treatmentData[treatmentData<bigTEst])+sum(treatmentData>bigTEst)*log(lambdatEst)+sum(treatmentData>bigTEst)*bigTEst*(lambdatEst-lambdacEst)-lambdatEst*sum(treatmentData[treatmentData>bigTEst])


  #Returning the negative ll as this is what the mle() function expects
 return(-loglikelihood)
}

est <- optimize(f = nll, interval=c(1,20))

bigTvec <- seq(0, 20, by=0.001)
bigTll <- rep(NA, length(bigTvec))
for (j in 1:length(bigTvec)){
  bigTll[j] <- nll(bigTvec[j])
}

plot(bigTvec, bigTll)

bigTvec[which.min(bigTll)]

modelstring="

data {
  for (j in 1:m){
    zeros[j] <- 0
  }
}

model {
  C <- 10000
  for (i in 1:n){
    zeros[i] ~ dpois(zeros.mean[i])
    zeros.mean[i] <-  -l[i] + C
    l[i] <- ifelse(datEvent[i]==1, log(lambdac)-lambdac*datTimes[i], -lambdac*datTimes[i])
  }
  for (i in (n+1):m){                                                                                                             
    zeros[i] ~ dpois(zeros.mean[i])
    zeros.mean[i] <-  -l[i] + C
    l[i] <- ifelse(datEvent[i]==1, ifelse(datTimes[i]<bigT, log(lambdac)-lambdac*(datTimes[i]), log(lambdat)-lambdac*bigT-lambdat*(datTimes[i]-bigT)), 
      ifelse(datTimes[i]<bigT, -lambdac*datTimes[i], -lambdac*bigT-lambdat*(datTimes[i]-bigT)))
  }
  
    lambdac ~ dnorm(0.08,0.1)T(0,)
    lambdat ~ dnorm(0.04,0.1)T(0,)
    bigT ~ dnorm(6,0.1)T(0,)
     
    }
"

model = jags.model(textConnection(modelstring), data = list(datTimes = combinedData$time, datEvent = combinedData$event, n= n, m=m))

update(model, n.iter=1000)
output=coda.samples(model=model, variable.names=c("lambdac", "lambdat", "bigT"), n.iter = 10000)


summary(output)
plot(output)

#How do we use the output to simulate future observations?

# bigTobs <- output[[1]][,1]
# lambdacobs <- output[[1]][,2]
# lambdatobs <- output[[1]][,3]
# 
# 
# IAvec <- rep(NA, 100)
# for (i in 1:100){
#     bigTvec <- sample(bigTobs,1)
#     lambdacvec <- sample(lambdacobs,1)
#     lambdatvec <- sample(lambdatobs,1)
#     
#     
#     numberofcontroleventsremaining <- combinedData %>%
#       filter(group=="control") %>%
#       count(cens==TRUE) %>%
#        pull(n) %>%
#        nth(1)
#     
#     numberoftreatmenteventsremaining <- combinedData %>%
#       filter(group=="treatment") %>%
#       count(cens==TRUE) %>%
#        pull(n) %>%
#        nth(1)
#     
#     remainingcontroltimes <- -(1/lambdacvec)*log(runif(numberofcontroleventsremaining, 0, exp(-lambdacvec*IATime)))
#     
#     CP <- exp(-lambdacvec*bigTvec)
#     
#     Survleft <- ifelse(bigTvec<IATime, exp(-lambdacvec*bigTvec - lambdatvec*(IATime-bigTvec)), exp(-lambdacvec*IATime))
#     
#     remainingtreatmentsurvs <- runif(numberoftreatmenteventsremaining, 0, Survleft)
#     
#     remainingtreatmenttimes <- ifelse(remainingtreatmentsurvs<bigTvec, (1/lambdatvec)*(-log(remainingtreatmentsurvs)+bigTvec*(lambdatvec-lambdacvec)), -(1/lambdacvec)*log(remainingtreatmentsurvs))
#     
#     
#     combinedData <- combinedData %>%
#       filter(time<IATime)
#     
#     remainingData <- data.frame(time = c(remainingcontroltimes, remainingtreatmenttimes), group = c(rep("control", numberofcontroleventsremaining), rep("treatment", numberoftreatmenteventsremaining)), cens = rep(0, numberofcontroleventsremaining+numberoftreatmenteventsremaining))
#     
#     combinedData <- rbind(combinedData, remainingData)
#     
#     
#     
#     combinedData$cens <- combinedData$time<totalLength
#     
#     test <- survdiff(Surv(time, cens)~group, data = combinedData) 
#     IAvec[i] <- test$chisq>qchisq(0.95, 1)
#   
# }
# 
# mean(IAvec)
```

Now we have draws from the posterior distributions for $T$, $\lambda_c$ and $\lambda_t$, we can use these to simulate future observations. 




```{r, eval=F, echo=F}

lambda1 <- 0.04
lambda2 <- 0.08
bigT <- 6

#Sample sizes in each group
n1 <- 5000
n2 <- 5000

#Simulating the control and treatment data - again, we would not normally know the underlying structure, but for illustration this is okay
#Control
controlData <-rexp(n1, lambda2)
#Treatment
CP <- exp(-(lambda2*bigT))[[1]]
u <- runif(n2)
suppressWarnings(treatmentData <- ifelse(u>CP, -(1/lambda2)*log(u), (1/lambda1)*(-log(u)-bigT*(lambda2-lambda1))))

n <- length(controlData)
m <- length(treatmentData)


nll <- function(bigTEst, lambdacEst, lambdatEst){
   loglikelihood <-  sum(treatmentData<bigTEst)*log(lambdacEst)-lambdacEst*sum(treatmentData[treatmentData<bigTEst])+sum(treatmentData>bigTEst)*log(lambdatEst)+sum(treatmentData>bigTEst)*bigTEst*(lambdatEst-lambdacEst)-lambdatEst*sum(treatmentData[treatmentData>bigTEst])


  #Returning the negative ll as this is what the mle() function expects
 return(-loglikelihood)
}

est <- stats4::mle(minuslogl = nll, start = list(bigTEst=10, lambdacEst = 0.01, lambdatEst = 0.01), lower=  list(bigTEst=0.5, lambdacEst = 0.0001, lambdatEst = 0.0001), upper = list(bigTEst = 60))



est@coef


```




```{r, eval=F, echo=F}

lambda1 <- 0.04
lambda2 <- 0.08
bigT <- 6

#Sample sizes in each group
n1 <- 100
n2 <- 100

#Simulating the control and treatment data - again, we would not normally know the underlying structure, but for illustration this is okay
#Control
controlData <-rexp(n1, lambda2)
#Treatment
CP <- exp(-(lambda2*bigT))[[1]]
u <- runif(n2)
suppressWarnings(treatmentData <- ifelse(u>CP, -(1/lambda2)*log(u), (1/lambda1)*(-log(u)-bigT*(lambda2-lambda1))))

combinedData <- data.frame(time = c(controlData, treatmentData), group = c(rep("control", n1), rep("treatment", n2)))

kmfit <- survfit(Surv(time)~group, data = combinedData)
plot(kmfit, col=c("blue", "red"))

nll <- function(bigTEst){
  lambdacEst <- lambda2
  lambdatEst <- lambda1
   loglikelihood <-  sum(treatmentData<bigTEst)*log(lambdacEst)-lambdacEst*sum(treatmentData[treatmentData<bigTEst])+sum(treatmentData>bigTEst)*log(lambdatEst)+sum(treatmentData>bigTEst)*bigTEst*(lambdatEst-lambdacEst)-lambdatEst*sum(treatmentData[treatmentData>bigTEst])


  #Returning the negative ll as this is what the mle() function expects
 return(-loglikelihood)
}

bigTvec <- seq(0, 20, by=0.001)
bigTll <- rep(NA, length(bigTvec))
for (j in 1:length(bigTvec)){
  bigTll[j] <- nll(bigTvec[j])
}





est <- optimize(f = nll, interval=c(1,20))


y <- est$minimum
plot(bigTvec, bigTll)
abline(v = y)
abline(v = bigTvec[which.min(bigTll)])

```
























